"use strict";(self.webpackChunkuml_umbrello_tutorials=self.webpackChunkuml_umbrello_tutorials||[]).push([[450],{533:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"documentation/etl-process","title":"Building ETL Pipelines","description":"Understanding and implementing Extract, Transform, Load (ETL) pipelines for data processing","source":"@site/docs/documentation/etl-process.md","sourceDirName":"documentation","slug":"/documentation/etl-process","permalink":"/ACMW-Jump-Project/docs/documentation/etl-process","draft":false,"unlisted":false,"editUrl":"https://github.com/prakash-aryan/ACMW-Jump-Project/tree/main/docs/documentation/etl-process.md","tags":[],"version":"current","frontMatter":{"title":"Building ETL Pipelines","sidebar_label":"ETL Pipelines","description":"Understanding and implementing Extract, Transform, Load (ETL) pipelines for data processing"}}');var a=t(4848),r=t(8453);const i={title:"Building ETL Pipelines",sidebar_label:"ETL Pipelines",description:"Understanding and implementing Extract, Transform, Load (ETL) pipelines for data processing"},l="Building ETL Pipelines",o={},d=[{value:"ETL Process Components",id:"etl-process-components",level:2},{value:"1. Extract",id:"1-extract",level:3},{value:"2. Transform",id:"2-transform",level:3},{value:"3. Load",id:"3-load",level:3},{value:"Example ETL Pipeline in Python",id:"example-etl-pipeline-in-python",level:2},{value:"Best Practices for ETL Development",id:"best-practices-for-etl-development",level:2},{value:"Modern ETL Tools",id:"modern-etl-tools",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"building-etl-pipelines",children:"Building ETL Pipelines"})}),"\n",(0,a.jsx)(n.p,{children:"ETL (Extract, Transform, Load) is a fundamental data processing pattern used to collect data from various sources, transform it to meet business needs, and load it into a target destination like a data warehouse."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"ETL Process Diagram",src:t(2110).A+"",width:"945",height:"483"})}),"\n",(0,a.jsx)(n.h2,{id:"etl-process-components",children:"ETL Process Components"}),"\n",(0,a.jsx)(n.h3,{id:"1-extract",children:"1. Extract"}),"\n",(0,a.jsx)(n.p,{children:"The extract phase involves pulling data from source systems. These sources could be:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Relational databases"}),"\n",(0,a.jsx)(n.li,{children:"APIs"}),"\n",(0,a.jsx)(n.li,{children:"Flat files (CSV, JSON)"}),"\n",(0,a.jsx)(n.li,{children:"Web scraping"}),"\n",(0,a.jsx)(n.li,{children:"IoT devices"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-transform",children:"2. Transform"}),"\n",(0,a.jsx)(n.p,{children:"The transformation phase involves cleaning, validating, and restructuring the data to prepare it for analysis:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Data cleansing"}),"\n",(0,a.jsx)(n.li,{children:"Normalization"}),"\n",(0,a.jsx)(n.li,{children:"Aggregation"}),"\n",(0,a.jsx)(n.li,{children:"Filtering"}),"\n",(0,a.jsx)(n.li,{children:"Type conversion"}),"\n",(0,a.jsx)(n.li,{children:"Key generation"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-load",children:"3. Load"}),"\n",(0,a.jsx)(n.p,{children:"The loading phase involves writing the processed data to a destination system:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Data warehouses"}),"\n",(0,a.jsx)(n.li,{children:"Data lakes"}),"\n",(0,a.jsx)(n.li,{children:"Databases"}),"\n",(0,a.jsx)(n.li,{children:"Application data stores"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"example-etl-pipeline-in-python",children:"Example ETL Pipeline in Python"}),"\n",(0,a.jsx)(n.p,{children:"Here's a simple ETL pipeline using Python with Pandas for transforming CSV data:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef extract():\n    \"\"\"Extract data from CSV files\"\"\"\n    print(\"Extracting data...\")\n    \n    # Load sales data\n    sales_df = pd.read_csv('sales_data.csv')\n    \n    # Load customer data\n    customer_df = pd.read_csv('customer_data.csv')\n    \n    return sales_df, customer_df\n\ndef transform(sales_df, customer_df):\n    \"\"\"Transform the extracted data\"\"\"\n    print(\"Transforming data...\")\n    \n    # Clean sales data\n    sales_df = sales_df.dropna(subset=['transaction_id', 'product_id'])\n    sales_df['sale_date'] = pd.to_datetime(sales_df['sale_date'])\n    sales_df['amount'] = sales_df['amount'].astype(float)\n    \n    # Add calculated fields\n    sales_df['month'] = sales_df['sale_date'].dt.month\n    sales_df['year'] = sales_df['sale_date'].dt.year\n    \n    # Join with customer data\n    merged_df = pd.merge(\n        sales_df,\n        customer_df[['customer_id', 'customer_name', 'segment']],\n        on='customer_id',\n        how='left'\n    )\n    \n    # Calculate aggregations\n    summary_df = merged_df.groupby(['year', 'month', 'segment']).agg({\n        'amount': ['sum', 'mean', 'count'],\n        'transaction_id': pd.Series.nunique\n    }).reset_index()\n    \n    summary_df.columns = ['year', 'month', 'segment', 'total_sales', \n                         'average_sale', 'transaction_count', 'unique_transactions']\n    \n    return merged_df, summary_df\n\ndef load(merged_df, summary_df):\n    \"\"\"Load transformed data to destination\"\"\"\n    print(\"Loading data...\")\n    \n    # Save detailed data to parquet format\n    merged_df.to_parquet('processed_sales_data.parquet')\n    \n    # Save summary data to CSV\n    summary_df.to_csv('sales_summary.csv', index=False)\n    \n    # In a real scenario, you might load to a database\n    print(f\"Data successfully loaded. Processed {len(merged_df)} rows.\")\n    print(f\"Created summary with {len(summary_df)} rows.\")\n\ndef run_etl_pipeline():\n    \"\"\"Run the full ETL pipeline\"\"\"\n    start_time = datetime.now()\n    print(f\"Starting ETL pipeline at {start_time}\")\n    \n    # Extract\n    sales_df, customer_df = extract()\n    \n    # Transform\n    merged_df, summary_df = transform(sales_df, customer_df)\n    \n    # Load\n    load(merged_df, summary_df)\n    \n    end_time = datetime.now()\n    print(f\"ETL pipeline completed at {end_time}\")\n    print(f\"Total runtime: {end_time - start_time}\")\n\nif __name__ == \"__main__\":\n    run_etl_pipeline()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-for-etl-development",children:"Best Practices for ETL Development"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Idempotency"}),": Ensure your pipelines can be run multiple times without creating duplicate data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Handling"}),": Implement robust error handling and logging"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitoring"}),": Set up monitoring for your ETL processes with alerts"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Incremental Processing"}),": Process only new or changed data when possible"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Validation"}),": Validate input and output data with schema enforcement"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Version Control"}),": Keep your ETL code in version control"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Documentation"}),": Document data lineage and transformation logic"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"modern-etl-tools",children:"Modern ETL Tools"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Apache Airflow"}),": Workflow management platform"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Apache NiFi"}),": Data flow automation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AWS Glue"}),": Serverless ETL service"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Talend"}),": Open-source integration platform"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Informatica PowerCenter"}),": Enterprise data integration"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},2110:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/etl-831cfbc4630fa60192ec1ef52897e608.png"},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>l});var s=t(6540);const a={},r=s.createContext(a);function i(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);